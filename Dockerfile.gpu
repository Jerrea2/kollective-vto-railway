# syntax=docker/dockerfile:1.6

########################################
# STAGE 1 — Bake SDXL (Build-Time Only)
########################################
FROM python:3.10-slim AS sdxl_bake

ARG SDXL_REPO=stabilityai/stable-diffusion-xl-base-1.0
ARG SDXL_REVISION=main
ARG SDXL_DIR=/models/sdxl

RUN apt-get update && apt-get install -y --no-install-recommends \
    git ca-certificates \
    && rm -rf /var/lib/apt/lists/*

RUN pip install --no-cache-dir huggingface_hub==0.25.2

# Create bake script inside image
RUN echo 'import os' > /bake_sdxl.py && \
    echo 'from huggingface_hub import snapshot_download' >> /bake_sdxl.py && \
    echo 'repo_id = os.environ["SDXL_REPO"]' >> /bake_sdxl.py && \
    echo 'revision = os.environ["SDXL_REVISION"]' >> /bake_sdxl.py && \
    echo 'local_dir = os.environ["SDXL_DIR"]' >> /bake_sdxl.py && \
    echo 'token = os.environ["HF_TOKEN"]' >> /bake_sdxl.py && \
    echo 'snapshot_download(repo_id=repo_id, revision=revision, local_dir=local_dir, local_dir_use_symlinks=False, token=token)' >> /bake_sdxl.py && \
    echo 'print("SDXL baked into:", local_dir)' >> /bake_sdxl.py

RUN --mount=type=secret,id=HF_TOKEN \
    export HF_TOKEN=$(cat /run/secrets/HF_TOKEN) && \
    python /bake_sdxl.py

RUN test -f ${SDXL_DIR}/model_index.json

########################################
# STAGE 2 — GPU Runtime
########################################
FROM nvidia/cuda:11.8.0-runtime-ubuntu22.04

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    git \
    curl \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

RUN ln -sf /usr/bin/python3.10 /usr/bin/python

WORKDIR /app

COPY --from=sdxl_bake /models/sdxl /models/sdxl

ENV HF_HUB_OFFLINE=1
ENV TRANSFORMERS_OFFLINE=1
ENV HF_DATASETS_OFFLINE=1
ENV HF_HUB_DISABLE_TELEMETRY=1

COPY requirements.txt /app/requirements.txt
RUN pip install --no-cache-dir -r /app/requirements.txt

COPY src /app/src

ENV PYTHONUNBUFFERED=1
ENV PRETRAINED_MODEL_PATH=/models/sdxl

EXPOSE 8000

CMD ["python", "-m", "uvicorn", "src.inference:app", "--host", "0.0.0.0", "--port", "8000"]
